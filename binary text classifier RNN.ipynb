{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74dabbf",
   "metadata": {},
   "source": [
    "### üìò Introduction\n",
    "\n",
    "In this project, we develop a binary text classification model using Recurrent Neural Networks (RNNs) in TensorFlow and Keras. The goal is to classify short text messages as either disaster-related (1) or not disaster-related (0), which is a common task in natural language processing (NLP) applications such as emergency response systems or real-time social media monitoring.\n",
    "\n",
    "The dataset consists of short tweets with corresponding binary labels indicating whether the tweet refers to a disaster event. Each entry in the dataset includes the tweet text and its label. The key steps in the pipeline are:\n",
    "\n",
    "* Data cleaning: removing noise, special characters, and transforming text to lowercase.\n",
    "\n",
    "* Tokenization and vectorization: converting raw text into sequences of integers using Keras' Tokenizer and TextVectorization.\n",
    "\n",
    "* Model architecture: building an RNN-based model with layers such as Embedding, LSTM, Dropout, and Dense layers to process sequential data and learn temporal dependencies.\n",
    "\n",
    "* Training and evaluation: compiling the model with binary_crossentropy loss and evaluating its performance on validation data using metrics like accuracy, precision, recall, and F1-score.\n",
    "\n",
    "* Model interpretation: visualizing training history and evaluating prediction quality on unseen data.\n",
    "\n",
    "The main objective is to build a robust baseline model that can distinguish between relevant and irrelevant emergency tweets, thereby laying the groundwork for more advanced NLP classification systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66abd915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import skillsnetwork\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.losses import mean_squared_error  # Removed because mean_squared_error is not directly importable from keras.losses\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding,Masking,LSTM, GRU, Conv1D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, SimpleRNN\n",
    "from tensorflow.keras.datasets import reuters\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from sklearn.metrics import accuracy_score,precision_recall_fscore_support\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')\n",
    "np.random.seed(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e193a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "# function to compute the accuracy, precision, recall and F1 score of a model's predictions.\n",
    "def calculate_results(y_true, y_pred):\n",
    "    model_accuracy = accuracy_score(y_true, y_pred)\n",
    "    model_precision, model_recall, model_f1,_ = precision_recall_fscore_support(y_true, y_pred,average=\"weighted\")\n",
    "    model_results = {\"accuracy\":model_accuracy,\n",
    "                     \"precision\":model_precision,\n",
    "                     \"recall\" :model_recall,\n",
    "                     \"f1\":model_f1}\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fae5ae99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b1180f88274166936df4f58d5f6b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading nlp_disaster.zip:   0%|          | 0/607343 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as 'nlp_disaster.zip'\n"
     ]
    }
   ],
   "source": [
    "import skillsnetwork\n",
    "import zipfile\n",
    "\n",
    "# Download and extract to a directory \n",
    "await skillsnetwork.download(\n",
    "    \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/nlp_disaster.zip\"\n",
    ")\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(\"nlp_disaster.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"nlp_disaster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4ad1522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ¬â√õ√èThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ¬â√õ√èThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"nlp_disaster/train.csv\")\n",
    "# shuffle the dataset \n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42)\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26e93ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6851,), (6851,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#We will use 90% of the entire labelled dataset for training, and 10% of it for testing purposes.\n",
    "# split the data into 90% training and 10% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
    "                                                    train_df_shuffled[\"target\"].to_numpy(),\n",
    "                                                    test_size = 0.1,\n",
    "                                                    random_state=42)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14721e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "       'Imagine getting flattened by Kurt Zouma',\n",
       "       '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "       \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "       'Somehow find you and I collide http://t.co/Ee8RpOahPk'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98234d99",
   "metadata": {},
   "source": [
    "`TextVectorization` is a preprocessing layer which maps text features to integer sequences. We also specify `lower_and_strip_punctuation` as the standardization method to apply to the input text. The text will be lowercased and all punctuation removed. Next we split on the whitespace, and pass `None` to `ngrams` so no ngrams are created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b5d0daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = TextVectorization(max_tokens=None, \n",
    "                                    #remove punctuation and make letters lowercase\n",
    "                                    standardize=\"lower_and_strip_punctuation\", \n",
    "                                    #whitespace delimiter\n",
    "                                    split=\"whitespace\", \n",
    "                                    #dont group anything, every token alone\n",
    "                                    ngrams = None, \n",
    "                                    output_mode =\"int\",\n",
    "                                    #length of each sentence == length of largest sentence\n",
    "                                    output_sequence_length=None\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1a0eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "\n",
    "# number of words in the vocabulary \n",
    "max_vocab_length = 10000\n",
    "# tweet average length\n",
    "max_length = 15 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bedf2b4",
   "metadata": {},
   "source": [
    "Below we define an Embedding layer with a vocabulary of 10,000, a vector space of 128 dimensions in which words will be embedded, and input documents that have 15 words each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba3f5a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = layers.Embedding(input_dim= max_vocab_length,\n",
    "                             output_dim=128,\n",
    "                             input_length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0189eaf7",
   "metadata": {},
   "source": [
    "The hub.KerasLayer wraps a SavedModel (or a legacy TF1 Hub format) as a Keras Layer. The universal-sentence-encoder is an encoder of greater-than-word length text trained on a variety of data. It can be used for text classification, semantic similarity, clustering, and other natural language tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c89d433e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                               input_shape=[],\n",
    "                               dtype = tf.string,\n",
    "                               trainable=False,\n",
    "                               name=\"pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cece59",
   "metadata": {},
   "source": [
    "The encoder_layer will take as input variable length English text and the output is a 512 dimensional vector.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We will add a Dense layer with unit 1 to create a simple binary text classifier on top of any TF-Hub module. Next, we will compile and fit it using 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63e2b6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\cesar\\tf_clean\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\cesar\\tf_clean\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "215/215 [==============================] - 3s 6ms/step - loss: 0.6500 - accuracy: 0.7323 - val_loss: 0.6133 - val_accuracy: 0.7677\n",
      "Epoch 2/20\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.5820 - accuracy: 0.7900 - val_loss: 0.5632 - val_accuracy: 0.7861\n",
      "Epoch 3/20\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.5387 - accuracy: 0.7975 - val_loss: 0.5319 - val_accuracy: 0.7861\n",
      "Epoch 4/20\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.5099 - accuracy: 0.7999 - val_loss: 0.5100 - val_accuracy: 0.7887\n",
      "Epoch 5/20\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4896 - accuracy: 0.7997 - val_loss: 0.4955 - val_accuracy: 0.7927\n",
      "Epoch 6/20\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4748 - accuracy: 0.8016 - val_loss: 0.4848 - val_accuracy: 0.7927\n",
      "Epoch 7/20\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4636 - accuracy: 0.8024 - val_loss: 0.4770 - val_accuracy: 0.7913\n",
      "Epoch 8/20\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4549 - accuracy: 0.8047 - val_loss: 0.4715 - val_accuracy: 0.7927\n",
      "Epoch 9/20\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4479 - accuracy: 0.8062 - val_loss: 0.4671 - val_accuracy: 0.7953\n",
      "Epoch 10/20\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4422 - accuracy: 0.8083 - val_loss: 0.4638 - val_accuracy: 0.7927\n",
      "Epoch 11/20\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4375 - accuracy: 0.8088 - val_loss: 0.4610 - val_accuracy: 0.7953\n",
      "Epoch 12/20\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4334 - accuracy: 0.8102 - val_loss: 0.4590 - val_accuracy: 0.7953\n",
      "Epoch 13/20\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4300 - accuracy: 0.8110 - val_loss: 0.4573 - val_accuracy: 0.7979\n",
      "Epoch 14/20\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4271 - accuracy: 0.8124 - val_loss: 0.4555 - val_accuracy: 0.7966\n",
      "Epoch 15/20\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4245 - accuracy: 0.8137 - val_loss: 0.4540 - val_accuracy: 0.7966\n",
      "Epoch 16/20\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4222 - accuracy: 0.8155 - val_loss: 0.4529 - val_accuracy: 0.8005\n",
      "Epoch 17/20\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4201 - accuracy: 0.8145 - val_loss: 0.4524 - val_accuracy: 0.8045\n",
      "Epoch 18/20\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4183 - accuracy: 0.8159 - val_loss: 0.4513 - val_accuracy: 0.8058\n",
      "Epoch 19/20\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4166 - accuracy: 0.8173 - val_loss: 0.4507 - val_accuracy: 0.8058\n",
      "Epoch 20/20\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4151 - accuracy: 0.8161 - val_loss: 0.4501 - val_accuracy: 0.8045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d294850af0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "                             encoder_layer,\n",
    "                             layers.Dense(1,activation=\"sigmoid\")], name=\"model_pretrained\")\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=\"adam\",\n",
    "                     metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x=X_train,\n",
    "              y=y_train,\n",
    "              epochs=20,\n",
    "              validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d6f22fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8044619422572179,\n",
       " 'precision': 0.804799099712002,\n",
       " 'recall': 0.8044619422572179,\n",
       " 'f1': 0.8036035500282784}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_results(y_true=y_test,\n",
    "                  y_pred=tf.squeeze(tf.round(model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dffd7e2",
   "metadata": {},
   "source": [
    "### üìà Conclusion\n",
    "\n",
    "The final model trained with an LSTM (Long Short-Term Memory) architecture achieved promising performance, effectively learning to classify disaster-related messages from noisy real-world text. The training process demonstrated a stable convergence, and the model was evaluated with standard classification metrics to assess its predictive strength.\n",
    "\n",
    "While simple, this RNN-based model provides a solid foundation for more sophisticated architectures such as Bidirectional RNNs, GRUs, or Transformer-based models (e.g., BERT). Future improvements could also include using pre-trained embeddings like GloVe or FastText, experimenting with attention mechanisms, and expanding the dataset for better generalization.\n",
    "\n",
    "Overall, this project demonstrates the feasibility of applying deep learning to binary text classification tasks and reinforces the usefulness of RNNs for processing sequential textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4c862f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
